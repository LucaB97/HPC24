---
title: "High Performance Computing 2024 - Exercise 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The requirements for this exercise are described [here](https://github.com/Foundations-of-HPC/High-Performance-Computing-2023/blob/main/ASSIGNMENTS/exercise1/exercise1.md).\
In summary, the assignment consists of the the evaluation of different algorithms to perform collective operations in a cluster nodes.
The experiments are carried out using the [OSU benchmarks](https://mvapich.cse.ohio-state.edu/benchmarks/) for MPI operations.\

The software stack used in the project is :...  
\
\
Here is a summary of the contents of this report.\
• **Section 1** is about the benchmark functions and their parameters.\
• **Section 2** evaluates the choice of the mapping strategy.\
• **Section 3** specifies which operations and algorithms have been considered and describes their functioning from a theoretical point of view.\
• **Section 4** presents the results and compares the different algorithms' performances.\

## Section 1
The OSU benchmarks include a wide range of communication operations in different programming languages.\
The focus here is on host-based **blocking collective MPI benchmarks** in C.\
As an example, a function call for the *broadcast* operation is:\
`$mpirun --mca coll_tuned_use_dynamic_rules true --mca coll_tuned_bcast_algorithm 0 osu_bcast`,\
where `osu_bcast` is the file which implements the broadcast operation and `--mca coll_tuned_bcast_algorithm 0` specifies the choice of a specific algorithm.\
The output looks like this:\
![image](C:/Users/lucab/Desktop/Immagini HPC/osu_bcast_basic_output.png)\ \
One can retrieve the list of available algorithms (e.g.: `$ompi_info --param coll tuned --level 5 | grep "bcast"`).\
Conveniently, the OSU benchmark functions allow the setting of some parameters by the users.\
The parameter list is accessible through `$mpirun osu_bcast --help`:\
![image](C:/Users/lucab/Desktop/Immagini HPC/osu_help.png)
One should notice that the parameter lists may differ depending on the collective operations considered.\
A simple investigation, involving the parameters *i* (number of iteration for timing) and *w* (set number of warmup iterations to skip before timing), has been carried out to verify whether setting values different from the default ones would affect the results.\
A set of 10 experiments were performed, with a random selection of the operation, the algorithm and the mapping strategy. 
For each setting, the corresponding benchmark function was executed with every combination of values of `i={10000,50000}` and `w={200,1000}`.\
**insert graphs here**\
As one can notice from the graphs (the complete data is also included in the repository), modifying the values of these parameters does not yield consistent variations in the results, at least in the considered experiments.
Thus, from this point on, any further experiment will use the parameters with their default values. 

## Section 2
The way the processes are mapped to the computational resources is a crucial factor for the performance of the algorithms.\
In this section, we show some examples of this in the context of our experiment.\
**image: architecture of thin node**\
**image/table: mapping ranks to IDs**\
As expected, mapping by core fills all the CPUs in a certain NUMA region before starting filling the next NUMA, then moves to the next node; mapping by L3cache, socket and NUMA coincide in this specific architecture (since each node has exactly 2 sockets, 2 NUMA regions and 2 L3caches), in fact they alternatively assign processes to different sockets of a node, before starting the same procedure on the next one; mapping by node, finally, maps one process to each node and start over until all the processes have been assigned. 
As mentioned in [this](https://www.sciencedirect.com/science/article/pii/S0743731522000697?via%3Dihub) paper, at each step the time of communication is dominated by the longest communication. Most likely, the transmission of a message between nodes through a network channel take the longest; inside of a node, communications between CPUs in different NUMA regions takes longer than those in the same region.\
**image: latencies for point-to-point communication between the cores of the node**\
Different algorithms for a collective operation (e.g. broadcast) create different topologies, and the mapping strategy can actually impact the total communication time for a certain algorithm by affecting the point-to-point communications among CPUs.\
Below, some graphs are shown where for a set of broadcast algorithms the latencies obtained with different mappings are compared.\
**image: lantencies for algorithms with different mappings**\
In general, the biggest differences in the communication time based on the mapping occur for messages of bigger dimensions. Also, for any algorithm, using L3cache, NUMA or socket mappings produce intermediate results, with the core mapping and the node mapping getting the most extreme values.\
Then, specific observations can be done for each individual algorithm:\
- chain-tree algorithm: mapping by node increases the number of transmissions through the network channels, leading to higher communication times than the other mappings, for which only one inter-node communication is required (after all the first node has been filled).\
- knomial algorithm: again, mapping by node requires several communications through the network, while for the other mapping strategies the number of inter-node communications is bounded by *k* (they can happen either in the middle of a chain or in the communication from the root to the top of a chain).\
- binary-tree and binomial tree: here, the node mapping outperforms the others for big messages. The number of inter-node communications is the same for all the mapping strategies, but when mapping by node communication between nodes happens earlier in the broadcast process. The different results might be due to internal communication delays in each node due to resource contention among the processes, for the mappings where the inter-node communications occur just on the bottom of the tree.\




## Section 3

## Section 4
